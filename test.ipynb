{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 245.03265380859375\n",
      "Epoch 10, Loss: 241.9197540283203\n",
      "Epoch 20, Loss: 238.85108947753906\n",
      "Epoch 30, Loss: 235.80706787109375\n",
      "Epoch 40, Loss: 232.75558471679688\n",
      "Epoch 50, Loss: 229.70718383789062\n",
      "Epoch 60, Loss: 226.655517578125\n",
      "Epoch 70, Loss: 223.60711669921875\n",
      "Epoch 80, Loss: 220.5594482421875\n",
      "Epoch 90, Loss: 217.5092315673828\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class VariationalLinear(nn.Module):\n",
    "    \"\"\"A variational linear layer with mean field approximation\"\"\"\n",
    "    def __init__(self, input_features, output_features, prior_mean=0, prior_var=1):\n",
    "        super(VariationalLinear, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        \n",
    "        # Parameters for the means of the weights and biases\n",
    "        self.weight_mu = Parameter(torch.Tensor(output_features, input_features))\n",
    "        self.bias_mu = Parameter(torch.Tensor(output_features))\n",
    "        \n",
    "        # Parameters for the log variance of the weights and biases\n",
    "        self.weight_logvar = Parameter(torch.Tensor(output_features, input_features))\n",
    "        self.bias_logvar = Parameter(torch.Tensor(output_features))\n",
    "        \n",
    "        # Prior distributions for the weights and biases\n",
    "        self.prior_mean = prior_mean\n",
    "        self.prior_var = prior_var\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.weight_mu.data.normal_(0, 0.1)\n",
    "        self.bias_mu.data.zero_()\n",
    "        self.weight_logvar.data.fill_(-10)\n",
    "        self.bias_logvar.data.fill_(-10)\n",
    "    \n",
    "    def forward(self, x, sample=True):\n",
    "        if sample:\n",
    "            # Sample weights and biases from their distributions\n",
    "            weight = self.weight_mu + torch.randn_like(self.weight_mu) * torch.exp(0.5 * self.weight_logvar)\n",
    "            bias = self.bias_mu + torch.randn_like(self.bias_mu) * torch.exp(0.5 * self.bias_logvar)\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return F.linear(x, weight, bias)\n",
    "    \n",
    "    def kl_divergence(self):\n",
    "        # KL divergence between the posterior and the prior distribution\n",
    "        weight_var = torch.exp(self.weight_logvar)\n",
    "        bias_var = torch.exp(self.bias_logvar)\n",
    "        \n",
    "        kl_weight = 0.5 * torch.sum(weight_var / self.prior_var + (self.weight_mu - self.prior_mean)**2 / self.prior_var - 1. - self.weight_logvar + torch.log(self.prior_var))\n",
    "        kl_bias = 0.5 * torch.sum(bias_var / self.prior_var + (self.bias_mu - self.prior_mean)**2 / self.prior_var - 1. - self.bias_logvar + torch.log(self.prior_var))\n",
    "        return kl_weight + kl_bias\n",
    "\n",
    "class MFVI_NN(nn.Module):\n",
    "    \"\"\"A complex Bayesian Neural Network with flexible architecture\"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, no_train_samples=10, no_pred_samples=100, prior_mean=0, prior_var=1):\n",
    "        super(MFVI_NN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.no_train_samples = no_train_samples\n",
    "        self.no_pred_samples = no_pred_samples\n",
    "        \n",
    "        # Creating variational layers\n",
    "        all_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        for i in range(len(all_sizes) - 1):\n",
    "            self.layers.append(VariationalLinear(all_sizes[i], all_sizes[i+1], prior_mean, prior_var))\n",
    "    \n",
    "    def forward(self, x, sample=True):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x, sample))\n",
    "        x = self.layers[-1](x, sample)\n",
    "        return x\n",
    "    \n",
    "    def kl_divergence(self):\n",
    "        kl = 0\n",
    "        for layer in self.layers:\n",
    "            kl += layer.kl_divergence()\n",
    "        return kl\n",
    "\n",
    "# Assuming the model setup:\n",
    "input_size = 28 * 28  # Example for MNIST\n",
    "hidden_sizes = [512, 256]\n",
    "output_size = 10  # Example for MNIST classification\n",
    "\n",
    "model = MFVI_NN(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Define a dummy dataset\n",
    "x = torch.randn(64, input_features)\n",
    "y = torch.randn(64, output_features)\n",
    "\n",
    "# Loss function and optimizer\n",
    "optimizer = torch.optim.Adam(bnn.parameters(), lr=0.01)\n",
    "\n",
    "def train():\n",
    "    bnn.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = bnn(x)\n",
    "    reconstruction_loss = F.mse_loss(output, y)\n",
    "    kl_divergence = bnn.kl_divergence()\n",
    "    # The final loss combines the reconstruction loss and the KL divergence term\n",
    "    loss = reconstruction_loss + kl_divergence\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    loss = train()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [32, 10] but got: [50, 10].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Forward pass with a sample from the variational distribution\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_train_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Predictive loss\u001b[39;00m\n\u001b[1;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, Y_batch)\n",
      "File \u001b[0;32m~/miniconda3/envs/MDS/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[32], line 76\u001b[0m, in \u001b[0;36mMFVI_NN.forward\u001b[0;34m(self, x, samples)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 76\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     77\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x, samples)\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/MDS/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[32], line 49\u001b[0m, in \u001b[0;36mVariationalLayer.forward\u001b[0;34m(self, x, samples)\u001b[0m\n\u001b[1;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Perform batch matrix multiplication\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# The output will have the shape [batch_size, 1, out_features]\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Squeeze to remove the middle dimension and add the bias\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Now output has the shape [batch_size, out_features]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m bias\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [32, 10] but got: [50, 10]."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example dataset (replace these with real data)\n",
    "X_train = torch.randn(100, 10)  # 100 samples, 10 features each\n",
    "Y_train = torch.randint(0, 2, (100,))  # Binary targets for this example\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "no_train_samples = 10\n",
    "no_pred_samples = 100\n",
    "\n",
    "# Prior hyperparameters\n",
    "prior_mean = 0.0\n",
    "prior_var = 1.0\n",
    "\n",
    "# Create a dataset and dataloader\n",
    "dataset = TensorDataset(X_train, Y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model initialization\n",
    "model = MFVI_NN(input_size=10, hidden_sizes=[50, 50], output_size=2,\n",
    "                no_train_samples=no_train_samples, no_pred_samples=no_pred_samples,\n",
    "                prior_mean=prior_mean, prior_var=prior_var)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for X_batch, Y_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with a sample from the variational distribution\n",
    "        outputs = model(X_batch, samples=no_train_samples)\n",
    "        \n",
    "        # Predictive loss\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        \n",
    "        # KL divergence (regularization term)\n",
    "        kl_div = model.kl_divergence()\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + kl_div / X_train.size(0)  # Normalize KL divergence by the dataset size\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += total_loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(dataloader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
